{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from alphaPhosHelperFunctions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "import plotly.figure_factory as ff\n",
    "from PeptideCollapse import *\n",
    "import analytics_core_V04 as ac\n",
    "import kinase_library as kl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import gseapy as gp\n",
    "from gseapy import dotplot, barplot, enrichment_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_between_asterics(match):\n",
    "    return match.group(1).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_condition (data, condition_list, collapse_level = 'PG'):\n",
    "    \"\"\"\n",
    "    Assigns condition setup to collapsed pandas dataframe\n",
    "\n",
    "    :param pd.DataFrame data: collapsed dataframe\n",
    "    :param dict condition_list: condition setup for the experiment with condition groups as key and sample unique IDs as values  \n",
    "    :param str collapse_level: default 'PG' returns protein group level collapse, 'P' return protein level collapse (useful for functional site assignment)\n",
    "    :return pd.DataFrame data: collapsed pandas dataframe \n",
    "\n",
    "    Example::\n",
    "        condition_dataframe = set_condition (data = collapsed_dataframe, condition_list, collapse_level = 'PG')\n",
    "    \"\"\"\n",
    "    \n",
    "    if collapse_level == 'P':\n",
    "        index_cols = ['UPD_seq', 'PTM_localization', 'Protein_name', 'Protein_group', 'Gene_group',\n",
    "                      'PTM_Collapse_key', 'Protein_Collapse_key', 'kinase_sequence']\n",
    "    if collapse_level == 'PG':\n",
    "        index_cols = ['UPD_seq', 'PTM_localization', 'Protein_group', 'Gene_group', 'PTM_Collapse_key', 'kinase_sequence']\n",
    "    index_df = data[index_cols]\n",
    "    num_df = data.drop(index_cols, axis = 1).T\n",
    "    names = [name for name in list(num_df.index)]\n",
    "    nam = []\n",
    "    for name in names:\n",
    "        for el in name.split('_'):\n",
    "            for key, value in condition_list.items():\n",
    "                for val in value:\n",
    "                    if val == el:\n",
    "                        nam.append(el)\n",
    "    num_df.index = nam\n",
    "    if collapse_level == 'PG':\n",
    "        num_df.columns = index_df['PTM_Collapse_key']\n",
    "    if collapse_level == 'P':\n",
    "        num_df.columns = index_df['Protein_Collapse_key']\n",
    "    num_df['Condition'] = np.nan\n",
    "    tmp = pd.DataFrame()\n",
    "    tmp1 = []\n",
    "    for key, value in condition_list.items():\n",
    "        for val in value:\n",
    "            tmp = num_df[num_df.index == val]\n",
    "            tmp['Condition'] = key\n",
    "            tmp1.append(tmp)\n",
    "    tmp1 = pd.concat(tmp1)\n",
    "    num_df = tmp1\n",
    "    num_df['group'] = num_df['Condition']\n",
    "    num_df = num_df.drop('Condition', axis = 1)\n",
    "    num_df['subject'] = num_df.index\n",
    "    num_df['sample'] = num_df.index\n",
    "    num_df = num_df.reset_index().drop('index', axis = 1)\n",
    "    num_df.columns.name = ''\n",
    "    return(num_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhosphoAnalysis:\n",
    "    def __init__(self):\n",
    "        self.processor = PeptideCollapse()\n",
    "        self.condition_df = None\n",
    "        self.collapsed_data = None\n",
    "        self.formatted_data = None\n",
    "        \n",
    "    \n",
    "    def peptideCollapse(self, data, **kwargs):\n",
    "        \n",
    "        self.collapsed_data = self.processor.process_complete_pipeline(\n",
    "            data=data, \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        return self.collapsed_data \n",
    "        \n",
    "    def assign_condition_setup(self, condition_df=None):\n",
    "        \n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "        \n",
    "        df_copy = self.collapsed_data.copy()\n",
    "        \n",
    "        if condition_df is None:\n",
    "            condition_df = self.processor.get_precursor_condition_dataset() \n",
    "            \n",
    "            \n",
    "            print(\"Assign conditions to each sample\")\n",
    "        \n",
    "            for i, sample in enumerate(condition_df['Sample']):\n",
    "                condition = input(f\"Enter condition for '{sample}': \")\n",
    "                condition_df.loc[i, 'Condition'] = condition\n",
    "            \n",
    "            quant_cols = condition_df['Sample'].unique().tolist()\n",
    "            meta_cols = [col for col in df_copy.columns if col not in quant_cols]\n",
    "            quant_df, meta_df = df_copy[quant_cols].T, df_copy[meta_cols].T\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            quant_cols = condition_df['Sample'].unique().tolist()\n",
    "            meta_cols = [col for col in df_copy.columns if col not in quant_cols]\n",
    "            quant_df, meta_df = df_copy[quant_cols].T, df_copy[meta_cols].T\n",
    "        \n",
    "        tmp_dict = dict(zip(condition_df['Sample'], condition_df['Condition']))\n",
    "        quant_df.columns = meta_df.loc[\"PTM_Collapse_key\"]\n",
    "        quant_df['group'] = quant_df.index.map(tmp_dict)\n",
    "        quant_df['sample'] = (quant_df['group'] + '_' + (quant_df.groupby('group').cumcount() + 1).astype(str))\n",
    "        quant_df['subject'] = quant_df['sample']\n",
    "        \n",
    "        self.formatted_data = quant_df\n",
    "        self.condition_df = condition_df\n",
    "        \n",
    "        return quant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kinase_prediction (dataset, kinase_df, group1, group2, kinase_class = \"ser_thr\"):\n",
    "    import kinase_library as kl\n",
    "    ###### Preparing kinase format\n",
    "    kinase_df_copy = kinase_df.copy()\n",
    "    def replace_between_asterics(match):\n",
    "        return match.group(1).upper()\n",
    "    tmp1 = []\n",
    "    for el in kinase_df_copy['kinase_sequence'].tolist():\n",
    "        tmp = re.sub(r'\\*([^*]+)\\*', replace_between_asterics, el)\n",
    "        tmp1.append(tmp.replace('_', '').upper())\n",
    "        \n",
    "    kinase_df_copy['kinase_sequence'] = tmp1\n",
    "    kinase_df_copy['PTM_Collapse_key'] = kinase_df_copy['PTM_Collapse_key'].apply(lambda x: x.split('~')[1]).apply(lambda x: x.split('_')[0]) + '_' + 'p' + kinase_df_copy['PTM_Collapse_key'].apply(lambda x: x.split('~')[1]).apply(lambda x: x.split('_')[1])\n",
    "    ###### \n",
    "    dataset_copy = dataset.copy()\n",
    "    dataset_copy = dataset_copy[(dataset_copy['group'] == group1) | (dataset_copy['group'] == group2)]\n",
    "    dataset_copy = dataset_copy.loc[:, dataset_copy.isna().sum() <= 10]\n",
    "    dataset_copy = ac.imputation_normal_distribution(dataset_copy).reset_index()\n",
    "    ttest_result = ac.run_ttest(dataset_copy, group1, group2)\n",
    "    ttest_result['identifier'] = ttest_result['identifier'].apply(lambda x: x.split('~')[1]).apply(lambda x: x.split('_')[0]) + '_' + 'p' + ttest_result['identifier'].apply(lambda x: x.split('~')[1]).apply(lambda x: x.split('_')[1])\n",
    "    ttest_result = ttest_result.sort_values('identifier')\n",
    "    ttest_result.columns = ['PTM_Collapse_key', 'T-statistics', 'pvalue', 'mean_group1', 'mean_group2',\n",
    "        'std(group1)', 'std(group2)', 'log2FC', 'test', 'correction', 'padj',\n",
    "       'rejected', 'group1', 'group2', 'FC', '-log10 pvalue', 'Method']\n",
    "    kinases = pd.DataFrame()\n",
    "    kinases = kinase_df_copy.merge(ttest_result, on = 'PTM_Collapse_key')\n",
    "    kinases = kinases[['PTM_Collapse_key','kinase_sequence', 'log2FC', 'T-statistics', 'padj']]\n",
    "    kinases.columns = ['Phosphosites', 'Sequence', 'logFC', 't', 'adj.P.Val']\n",
    "    test = kl.DiffPhosData(kinases, lfc_col='logFC', seq_col='Sequence', pval_col='adj.P.Val', pval_thresh=0.05)\n",
    "    kin_type = kinase_class\n",
    "    method = 'percentile_rank'\n",
    "    thresh = 15\n",
    "    test1 = test.kinase_enrichment(kin_type=kin_type, kl_method=method, kl_thresh=thresh)\n",
    "    fin_df = test1.combined_enrichment_results\n",
    "    return fin_df, test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_normalize_data(df):\n",
    "    scaler = StandardScaler()\n",
    "    df_norm = pd.DataFrame(\n",
    "        scaler.fit_transform(df.T).T,\n",
    "        index=df.index,\n",
    "        columns=df.columns\n",
    "    )\n",
    "    return df_norm\n",
    "\n",
    "def perform_hierarchical_clustering(df_norm, method='ward', metric='euclidean'):\n",
    "\n",
    "    if metric == 'correlation':\n",
    "        \n",
    "        sample_distances = pdist(df_norm.T, metric='correlation')\n",
    "        linkage_samples = linkage(sample_distances, method='average')\n",
    "    else:\n",
    "        linkage_samples = linkage(df_norm.T, method=method, metric=metric)\n",
    "\n",
    "    if metric == 'correlation':\n",
    "        feature_distances = pdist(df_norm, metric='correlation')\n",
    "        linkage_features = linkage(feature_distances, method='average')\n",
    "    else:\n",
    "        linkage_features = linkage(df_norm, method=method, metric=metric)\n",
    "    \n",
    "    return linkage_samples, linkage_features\n",
    "\n",
    "def plot_clustermap(df_norm, figsize=(12, 10), cmap='rocket', \n",
    "                   method='ward', metric='euclidean', n_clusters=6, save_path=None):\n",
    "\n",
    "    linkage_samples, linkage_features = perform_hierarchical_clustering(\n",
    "        df_norm, method=method, metric=metric\n",
    "    )\n",
    "\n",
    "    sample_clusters = fcluster(linkage_samples, n_clusters, criterion='maxclust')\n",
    "    feature_clusters = fcluster(linkage_features, n_clusters, criterion='maxclust')\n",
    "\n",
    "    cluster_colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "    sample_cluster_colors = []\n",
    "    for cluster in sample_clusters:\n",
    "        sample_cluster_colors.append(cluster_colors[cluster - 1])\n",
    "\n",
    "    feature_cluster_colors = []\n",
    "    for cluster in feature_clusters:\n",
    "        feature_cluster_colors.append(cluster_colors[cluster - 1])\n",
    "\n",
    "\n",
    "    feature_cluster_palette = {i+1: cluster_colors[i] for i in range(n_clusters)}\n",
    "\n",
    "\n",
    "    feature_color_array = [feature_cluster_palette[cluster] for cluster in feature_clusters]\n",
    "\n",
    "    g = sns.clustermap(\n",
    "        df_norm,\n",
    "        method=method,\n",
    "        metric=metric,\n",
    "        cmap=cmap,\n",
    "        center=0,\n",
    "        figsize=figsize,\n",
    "        cbar_kws={'label': 'Z-score'},\n",
    "        xticklabels=True,\n",
    "        yticklabels=False if df_norm.shape[0] > 50 else True,\n",
    "        dendrogram_ratio=0.15,\n",
    "        colors_ratio=0.03,\n",
    "        row_colors=feature_color_array\n",
    "    )\n",
    "    \n",
    "    if df_norm.shape[0] <= 100:\n",
    "        for i, cluster in enumerate(np.unique(feature_clusters)):\n",
    "            cluster_positions = np.where(feature_clusters == cluster)[0]\n",
    "            if len(cluster_positions) > 0:\n",
    "                center_pos = np.mean(cluster_positions)\n",
    "                g.ax_row_colors.text(\n",
    "                    0.5, center_pos, f'C{cluster}', \n",
    "                    ha='center', va='center', fontsize=8, fontweight='bold'\n",
    "                )\n",
    "\n",
    "    plt.setp(g.ax_heatmap.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=cluster_colors[i], \n",
    "                                   label=f'Cluster {i+1}') for i in range(n_clusters)]\n",
    "    \n",
    "\n",
    "    g.fig.legend(handles=legend_elements, title='Clusters', \n",
    "                bbox_to_anchor=(1.02, 0.8), loc='upper left')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    cluster_info = {\n",
    "        'sample_clusters': pd.DataFrame({'Sample': df_norm.columns, 'Cluster': sample_clusters}),\n",
    "        'feature_clusters': pd.DataFrame({'Feature': df_norm.index, 'Cluster': feature_clusters}),\n",
    "        'clustermap': g\n",
    "    }\n",
    "    \n",
    "    return cluster_info\n",
    "\n",
    "def extract_clusters(linkage_matrix, labels, n_clusters=6, threshold=None):\n",
    "    \n",
    "    if threshold:\n",
    "        clusters = fcluster(linkage_matrix, threshold, criterion='distance')\n",
    "    else:\n",
    "        clusters = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    cluster_df = pd.DataFrame({\n",
    "        'Item': labels,\n",
    "        'Cluster': clusters\n",
    "    })\n",
    "    \n",
    "    return cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gene_lists_from_clusters(feature_clusters, gene_column):\n",
    "    \n",
    "    if gene_column !=None and gene_column not in feature_clusters.columns:\n",
    "\n",
    "        feature_clusters['gene_symbol'] = feature_clusters[gene_column].apply(lambda x: x.split('~')[1]).apply(lambda x: x.split('_')[0])\n",
    "        for i in range(min(5, len(feature_clusters))):\n",
    "            original = feature_clusters.iloc[i][gene_column]\n",
    "            extracted = feature_clusters.iloc[i]['gene_symbol']\n",
    "            print(f\"  {original} -> {extracted}\")\n",
    "\n",
    "        failed_extractions = feature_clusters['gene_symbol'].isna().sum()\n",
    "        if failed_extractions > 0:\n",
    "            print(f\"\\nWarning: {failed_extractions} features could not be parsed for gene symbols\")\n",
    "            print(\"Examples of failed extractions:\")\n",
    "            failed_examples = feature_clusters[feature_clusters['gene_symbol'].isna()][gene_column].head(3).tolist()\n",
    "            for example in failed_examples:\n",
    "                print(f\"  {example}\")\n",
    "            \n",
    "\n",
    "            mask = feature_clusters['gene_symbol'].isna()\n",
    "\n",
    "            alternative_pattern = feature_clusters.loc[mask, gene_column].str.extract(r'([A-Z][A-Z0-9]+)')\n",
    "            feature_clusters.loc[mask, 'gene_symbol'] = alternative_pattern[0]\n",
    "            \n",
    "            remaining_failed = feature_clusters['gene_symbol'].isna().sum()\n",
    "            print(f\"After alternative extraction: {remaining_failed} still failed\")\n",
    "    \n",
    "    cluster_gene_dict = {}\n",
    "    \n",
    "    for cluster_id in sorted(feature_clusters['Cluster'].unique()):\n",
    "        genes_in_cluster = feature_clusters[\n",
    "            feature_clusters['Cluster'] == cluster_id\n",
    "        ][gene_column].tolist()\n",
    "\n",
    "        genes_in_cluster = [g for g in genes_in_cluster if pd.notna(g)]\n",
    "        genes_in_cluster = list(set(genes_in_cluster)) \n",
    "        \n",
    "        cluster_gene_dict[f'Cluster_{cluster_id}'] = genes_in_cluster\n",
    "        print(f\"Cluster {cluster_id}: {len(genes_in_cluster)} unique genes\")\n",
    "    \n",
    "    return cluster_gene_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_go_enrichment_per_cluster(cluster_gene_dict, organism='human', \n",
    "                                 gene_sets=['GO_Biological_Process_2023',\n",
    "                                          'GO_Molecular_Function_2023',\n",
    "                                          'GO_Cellular_Component_2023'],\n",
    "                                 cutoff=0.05, top_terms=20):\n",
    "    \"\"\"\n",
    "    Run GO enrichment analysis for each cluster\n",
    "    \n",
    "    Parameters:\n",
    "    cluster_gene_dict: dictionary with cluster names as keys and gene lists as values\n",
    "    organism: organism name ('human', 'mouse', etc.) - lowercase\n",
    "    gene_sets: list of gene set databases to use\n",
    "    cutoff: adjusted p-value cutoff\n",
    "    top_terms: number of top terms to keep per cluster\n",
    "    \n",
    "    Returns:\n",
    "    enrichment_results: dictionary with enrichment results for each cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    enrichment_results = {}\n",
    "    \n",
    "    for cluster_name, gene_list in cluster_gene_dict.items():\n",
    "        print(f\"\\nRunning enrichment for {cluster_name} ({len(gene_list)} genes)...\")\n",
    "        \n",
    "        if len(gene_list) < 3:\n",
    "            print(f\"Skipping {cluster_name}: too few genes ({len(gene_list)})\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Run enrichment analysis - removed unsupported parameters\n",
    "            enr = gp.enrichr(\n",
    "                gene_list=gene_list,\n",
    "                gene_sets=gene_sets,\n",
    "                organism=organism,  # 'human', 'mouse', 'yeast', etc.\n",
    "                cutoff=cutoff  # adjusted p-value cutoff\n",
    "            )\n",
    "            \n",
    "            # Get results and filter top terms\n",
    "            results_df = enr.results\n",
    "            \n",
    "            if not results_df.empty:\n",
    "                # Sort by adjusted p-value and keep top terms\n",
    "                results_df = results_df.sort_values('Adjusted P-value').head(top_terms)\n",
    "                enrichment_results[cluster_name] = {\n",
    "                    'results': results_df,\n",
    "                    'enrichr_object': enr\n",
    "                }\n",
    "                print(f\"Found {len(results_df)} significant terms for {cluster_name}\")\n",
    "            else:\n",
    "                print(f\"No significant terms found for {cluster_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error running enrichment for {cluster_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return enrichment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gsea_analysis(expression_data, gene_ranking_method='signal_to_noise', \n",
    "                     gene_sets=['GO_Biological_Process_2023'], \n",
    "                     classes=None, permutation_num=1000):\n",
    "    \"\"\"\n",
    "    Run Gene Set Enrichment Analysis (GSEA) on expression data\n",
    "    \n",
    "    Parameters:\n",
    "    expression_data: DataFrame with genes as rows, samples as columns\n",
    "    gene_ranking_method: method to rank genes ('signal_to_noise', 'log2_ratio_of_classes', etc.)\n",
    "    gene_sets: gene set databases to use\n",
    "    classes: sample class labels for comparison (e.g., ['control', 'treatment', ...])\n",
    "    permutation_num: number of permutations\n",
    "    \n",
    "    Returns:\n",
    "    gsea_results: GSEA results object\n",
    "    \"\"\"\n",
    "    \n",
    "    if classes is None:\n",
    "        print(\"Please provide class labels for GSEA analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Running GSEA analysis on {expression_data.shape[0]} genes...\")\n",
    "    \n",
    "    try:\n",
    "        # Run GSEA\n",
    "        gsea_results = gp.gsea(\n",
    "            data=expression_data,\n",
    "            gene_sets=gene_sets,\n",
    "            cls=classes,\n",
    "            method=gene_ranking_method,\n",
    "            permutation_num=permutation_num,\n",
    "            outdir=None,  # Don't save to disk\n",
    "            seed=42,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(f\"GSEA completed. Found {len(gsea_results.res2d)} gene sets.\")\n",
    "        return gsea_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running GSEA: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def plot_enrichment_results(enrichment_results, top_n=10, figsize=(12, 8), save_path=None):\n",
    "    \"\"\"\n",
    "    Create visualization of GO enrichment results\n",
    "    \n",
    "    Parameters:\n",
    "    enrichment_results: dictionary with enrichment results from run_go_enrichment_per_cluster\n",
    "    top_n: number of top terms to show per cluster\n",
    "    figsize: figure size\n",
    "    save_path: path to save the figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    \n",
    "    for cluster_name, results in enrichment_results.items():\n",
    "        df = results['results'].head(top_n).copy()\n",
    "        df['Cluster'] = cluster_name\n",
    "        df['-log10(adj_pval)'] = -np.log10(df['Adjusted P-value'])\n",
    "        plot_data.append(df)\n",
    "    \n",
    "    if not plot_data:\n",
    "        print(\"No enrichment results to plot\")\n",
    "        return\n",
    "    \n",
    "    combined_df = pd.concat(plot_data, ignore_index=True)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Dot plot style visualization\n",
    "    sns.scatterplot(\n",
    "        data=combined_df,\n",
    "        x='Cluster',\n",
    "        y='Term',\n",
    "        size='-log10(adj_pval)',\n",
    "        hue='Gene_set',\n",
    "        sizes=(50, 400),\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    plt.title('GO Term Enrichment Across Clusters', fontsize=16, pad=20)\n",
    "    plt.xlabel('Cluster', fontsize=12)\n",
    "    plt.ylabel('GO Terms', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def create_enrichment_heatmap(enrichment_results, top_n=15, figsize=(14, 10), save_path=None):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing enrichment across clusters\n",
    "    \n",
    "    Parameters:\n",
    "    enrichment_results: enrichment results dictionary\n",
    "    top_n: number of top terms to include\n",
    "    figsize: figure size\n",
    "    save_path: path to save figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all unique terms across clusters\n",
    "    all_terms = set()\n",
    "    cluster_term_pvals = {}\n",
    "    \n",
    "    for cluster_name, results in enrichment_results.items():\n",
    "        df = results['results'].head(top_n)\n",
    "        terms = df['Term'].tolist()\n",
    "        pvals = df['Adjusted P-value'].tolist()\n",
    "        \n",
    "        all_terms.update(terms)\n",
    "        cluster_term_pvals[cluster_name] = dict(zip(terms, pvals))\n",
    "    \n",
    "    # Create matrix for heatmap\n",
    "    all_terms = list(all_terms)\n",
    "    clusters = list(enrichment_results.keys())\n",
    "    \n",
    "    # Initialize matrix with NaN (will show as white/no enrichment)\n",
    "    matrix = np.full((len(all_terms), len(clusters)), np.nan)\n",
    "    \n",
    "    for j, cluster in enumerate(clusters):\n",
    "        for i, term in enumerate(all_terms):\n",
    "            if term in cluster_term_pvals[cluster]:\n",
    "                # Use -log10(p-value) for intensity\n",
    "                pval = cluster_term_pvals[cluster][term]\n",
    "                matrix[i, j] = -np.log10(pval) if pval > 0 else 10\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Mask NaN values to show them as white\n",
    "    mask = np.isnan(matrix)\n",
    "    \n",
    "    sns.heatmap(\n",
    "        matrix,\n",
    "        xticklabels=clusters,\n",
    "        yticklabels=all_terms,\n",
    "        cmap='Reds',\n",
    "        mask=mask,\n",
    "        cbar_kws={'label': '-log10(Adjusted P-value)'},\n",
    "        square=False\n",
    "    )\n",
    "    \n",
    "    plt.title('GO Term Enrichment Heatmap Across Clusters', fontsize=16, pad=20)\n",
    "    plt.xlabel('Clusters', fontsize=12)\n",
    "    plt.ylabel('GO Terms', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_phospho_median(phospho_df, protein_df, return_non_matched=False):\n",
    "    \"\"\"\n",
    "    Normalize phosphoproteomics data using condition-level protein-specific normalization.\n",
    "\n",
    "    This function normalizes each phosphosite by subtracting the median abundance of its\n",
    "    corresponding parent protein within each condition. Samples with the same name are\n",
    "    treated as replicates of the same condition. This approach preserves condition-specific\n",
    "    protein expression changes while providing phosphorylation stoichiometry information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    phospho_df : pandas.DataFrame\n",
    "        Phosphoproteomics dataframe with samples as index and phosphosites as columns.\n",
    "        Column names should contain protein identifiers (e.g., 'P12345~PROTEIN_S123').\n",
    "        Values should be log-transformed intensities. Duplicate sample names indicate\n",
    "        replicates of the same condition.\n",
    "    protein_df : pandas.DataFrame\n",
    "        Proteomics dataframe with samples as index and proteins as columns.\n",
    "        Column names should contain protein identifiers matching phospho data.\n",
    "        Values should be log-transformed intensities. Duplicate sample names indicate\n",
    "        replicates of the same condition.\n",
    "    return_non_matched : bool, optional\n",
    "        If True, returns all phosphosites including those without protein matches (unnormalized).\n",
    "        If False (default), returns only successfully normalized phosphosites.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing:\n",
    "        - 'normalized_phospho': DataFrame with protein-normalized phospho data\n",
    "        - 'condition_protein_medians': DataFrame with median protein values per condition\n",
    "        - 'phospho_to_protein_mapping': Dict mapping phosphosites to protein IDs\n",
    "        - 'normalization_success_rate': Float indicating fraction successfully normalized\n",
    "        - 'common_conditions': List of conditions present in both datasets\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Protein IDs are extracted from phosphosite names using '~' or '_' delimiters\n",
    "    - For each condition, calculates median protein abundance across replicates\n",
    "    - Each phosphosite normalized by its parent protein's median in that condition\n",
    "    - By default, only returns phosphosites with successful protein matches\n",
    "    - Set return_non_matched=True to include unmatched phosphosites (unnormalized)\n",
    "    - Preserves biological protein expression differences between conditions\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If no common conditions are found between the two datasets\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> results = normalize_phospho_median(phospho_data, protein_data)\n",
    "    >>> normalized_data = results['normalized_phospho']\n",
    "    >>> success_rate = results['normalization_success_rate']\n",
    "    >>> print(f\"Successfully normalized {success_rate:.1%} of phosphosites\")\n",
    "    \"\"\"\n",
    "    # Find common conditions\n",
    "    phospho_conditions = set(phospho_df.index)\n",
    "    protein_conditions = set(protein_df.index)\n",
    "    common_conditions = phospho_conditions & protein_conditions\n",
    "\n",
    "    if len(common_conditions) == 0:\n",
    "        raise ValueError(\"No common conditions found between phospho and protein data!\")\n",
    "\n",
    "    print(f\"Found {len(common_conditions)} common conditions: {sorted(common_conditions)}\")\n",
    "\n",
    "    # Filter to common conditions\n",
    "    phospho_matched = phospho_df.loc[phospho_df.index.isin(common_conditions)]\n",
    "    protein_matched = protein_df.loc[protein_df.index.isin(common_conditions)]\n",
    "\n",
    "    print(f\"Phospho samples: {len(phospho_matched)}\")\n",
    "    print(f\"Protein samples: {len(protein_matched)}\")\n",
    "\n",
    "    # Extract protein IDs from phosphosite names\n",
    "    def extract_protein_id(phosphosite_name):\n",
    "        \"\"\"Extract protein ID from phosphosite name (format: 'A0A087WUL8~NBPF19_S364_M1')\"\"\"\n",
    "        return (\n",
    "            phosphosite_name.split(\"~\")[0]\n",
    "            if \"~\" in phosphosite_name\n",
    "            else phosphosite_name.split(\"_\")[0]\n",
    "        )\n",
    "\n",
    "    # Create phosphosite to protein mapping\n",
    "    phospho_to_protein = {}\n",
    "    for phosphosite in phospho_matched.columns:\n",
    "        protein_id = extract_protein_id(phosphosite)\n",
    "        phospho_to_protein[phosphosite] = protein_id\n",
    "\n",
    "    print(f\"Mapped {len(phospho_to_protein)} phosphosites to proteins\")\n",
    "\n",
    "    # Calculate condition-level protein medians\n",
    "    # Group protein data by condition and calculate median for each protein\n",
    "    condition_protein_medians = protein_matched.groupby(protein_matched.index).median()\n",
    "\n",
    "    print(f\"Calculated protein medians for {len(condition_protein_medians)} conditions\")\n",
    "    print(f\"Protein medians shape: {condition_protein_medians.shape}\")\n",
    "\n",
    "    # Normalize phospho data\n",
    "    normalized_phospho = phospho_matched.copy()\n",
    "    successfully_normalized_phosphosites = set()\n",
    "    normalization_stats = {\n",
    "        \"total_values\": 0,\n",
    "        \"normalized\": 0,\n",
    "        \"protein_not_found\": 0,\n",
    "        \"missing_protein_values\": 0,\n",
    "    }\n",
    "\n",
    "    for condition in common_conditions:\n",
    "        condition_mask = phospho_matched.index == condition\n",
    "        condition_phospho = phospho_matched.loc[condition_mask]\n",
    "\n",
    "        for phosphosite in phospho_matched.columns:\n",
    "            protein_id = phospho_to_protein[phosphosite]\n",
    "            normalization_stats[\"total_values\"] += sum(condition_mask)\n",
    "\n",
    "            # Find matching protein column\n",
    "            matching_proteins = [\n",
    "                col for col in condition_protein_medians.columns if protein_id in col.split(\";\")\n",
    "            ]\n",
    "\n",
    "            if matching_proteins:\n",
    "                protein_col = matching_proteins[0]\n",
    "                condition_protein_median = condition_protein_medians.loc[condition, protein_col]\n",
    "\n",
    "                if not pd.isna(condition_protein_median):\n",
    "                    # Normalize all replicates of this condition for this phosphosite\n",
    "                    normalized_phospho.loc[condition_mask, phosphosite] = (\n",
    "                        condition_phospho[phosphosite] - condition_protein_median\n",
    "                    )\n",
    "                    normalization_stats[\"normalized\"] += sum(condition_mask)\n",
    "                    successfully_normalized_phosphosites.add(phosphosite)\n",
    "                else:\n",
    "                    normalization_stats[\"missing_protein_values\"] += sum(condition_mask)\n",
    "            else:\n",
    "                normalization_stats[\"protein_not_found\"] += sum(condition_mask)\n",
    "\n",
    "    # Filter to only successfully normalized phosphosites (unless return_non_matched=True)\n",
    "    if not return_non_matched:\n",
    "        print(\"Filtering to only successfully normalized phosphosites...\")\n",
    "        print(f\"Original phosphosites: {len(phospho_matched.columns)}\")\n",
    "        print(f\"Successfully normalized: {len(successfully_normalized_phosphosites)}\")\n",
    "        print(\n",
    "            f\"Removed: {len(phospho_matched.columns) - len(successfully_normalized_phosphosites)}\"\n",
    "        )\n",
    "\n",
    "        normalized_phospho = normalized_phospho[list(successfully_normalized_phosphosites)]\n",
    "\n",
    "        # Also update the original phospho for consistency\n",
    "        phospho_matched = phospho_matched[list(successfully_normalized_phosphosites)]\n",
    "\n",
    "    # Calculate success rate\n",
    "    success_rate = (\n",
    "        normalization_stats[\"normalized\"] / normalization_stats[\"total_values\"]\n",
    "        if normalization_stats[\"total_values\"] > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    print(\"\\nNormalization statistics:\")\n",
    "    print(f\"  Total phosphosite-sample combinations: {normalization_stats['total_values']:,}\")\n",
    "    print(f\"  Successfully normalized: {normalization_stats['normalized']:,} ({success_rate:.1%})\")\n",
    "    print(f\"  Protein not found: {normalization_stats['protein_not_found']:,}\")\n",
    "    print(f\"  Missing protein values: {normalization_stats['missing_protein_values']:,}\")\n",
    "\n",
    "    return {\n",
    "        \"normalized_phospho\": normalized_phospho,\n",
    "        \"condition_protein_medians\": condition_protein_medians,\n",
    "        \"phospho_to_protein_mapping\": phospho_to_protein,\n",
    "        \"normalization_success_rate\": success_rate,\n",
    "        \"common_conditions\": list(common_conditions),\n",
    "        \"original_phospho\": phospho_matched,\n",
    "        \"original_protein\": protein_matched,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_hela_nanophos_100cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_new_HeLa_sorted_100cells_Report.tsv', sep = '\\t')\n",
    "sorted_hela_nanophos_300cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_new_HeLa_sorted_300cells_Report.tsv', sep = '\\t')\n",
    "sorted_hela_nanophos_500cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_new_HeLa_sorted_500cells_Report.tsv', sep = '\\t')\n",
    "sorted_hela_nanophos_1000cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_new_HeLa_sorted_1000cells_Report.tsv', sep = '\\t')\n",
    "sorted_hela_nanophos_2000cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_new_HeLa_sorted_2000cells_Report.tsv', sep = '\\t')\n",
    "sorted_hela_nanophos_3000cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_new_HeLa_sorted_3000cells_Report.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_hela_uphos_100cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\uPhos_HeLa_sorted_100cells_Report.tsv', sep = '\\t')\n",
    "sorted_hela_uphos_300cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\uPhos_HeLa_sorted_300cells_Report.tsv', sep = '\\t')\n",
    "sorted_hela_uphos_500cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\uPhos_HeLa_sorted_500cells_Report.tsv', sep = '\\t')\n",
    "sorted_hela_uphos_1000cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\uPhos_HeLa_sorted_1000cells_Report.tsv', sep = '\\t')\n",
    "sorted_hela_uphos_2000cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\uPhos_HeLa_sorted_2000cells_Report.tsv', sep = '\\t')\n",
    "sorted_hela_uphos_3000cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\uPhos_HeLa_sorted_3000cells_Report.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_stemcells_100cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_StemCells_100cells_Report.tsv', sep = '\\t')\n",
    "sorted_stemcells_300cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_StemCells_300cells_Report.tsv', sep = '\\t')\n",
    "sorted_stemcells_500cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_StemCells_500cells_Report.tsv', sep = '\\t')\n",
    "sorted_stemcells_800cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_StemCells_800cells_Report.tsv', sep = '\\t')\n",
    "sorted_stemcells_1000cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_StemCells_1000cells_Report.tsv', sep = '\\t')\n",
    "sorted_stemcells_3000cells = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_StemCells_3000cells_Report.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\StemCells_RAlin_SL_all_Report.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemcells_500cells_normalized = pd.read_csv(r'W:\\User\\Denys\\SN_diffs\\nanoPhos_submission\\figure3\\nanoPhos_StemCells_500cells_RAlin_SL_normalized_Report.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_hela_sorted = [sorted_hela_nanophos_100cells, sorted_hela_nanophos_300cells, sorted_hela_nanophos_500cells, sorted_hela_nanophos_1000cells, sorted_hela_nanophos_2000cells, sorted_hela_nanophos_3000cells]\n",
    "l_hela_sorted_uphos = [sorted_hela_uphos_100cells, sorted_hela_uphos_300cells, sorted_hela_uphos_500cells, sorted_hela_uphos_1000cells, sorted_hela_uphos_2000cells, sorted_hela_uphos_3000cells]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_stemcells = [sorted_stemcells_100cells, sorted_stemcells_300cells, sorted_stemcells_500cells, \n",
    "               sorted_stemcells_800cells, sorted_stemcells_1000cells, sorted_stemcells_3000cells]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PeptideCollapse on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PeptideCollapse()\n",
    "l_hela_sorted_collapsed = []\n",
    "for l in l_hela_sorted:\n",
    "    l_hela_sorted_collapsed.append(pc.process_complete_pipeline(l, cutoff = 0, fasta_path=r'D:\\Projects\\Spectral libraries\\human.fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_stemcells_collapsed = []\n",
    "pc = PeptideCollapse()\n",
    "for l in l_stemcells:\n",
    "    l_stemcells_collapsed.append(pc.process_complete_pipeline(l, cutoff = 0, add_kinase_sequences=False))\n",
    "    \n",
    "####\n",
    "l_stemcells_collapsed_with_fasta = []\n",
    "pc = PeptideCollapse()\n",
    "for l in l_stemcells:\n",
    "    l_stemcells_collapsed_with_fasta.append(pc.process_complete_pipeline(l, cutoff = 0, fasta_path = r'D:\\Projects\\Spectral libraries\\mouse.fasta', kinase_window_size=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PeptideCollapse()\n",
    "l_hela_sorted_collapsed_uphos = []\n",
    "for l in l_hela_sorted_uphos:\n",
    "    l_hela_sorted_collapsed_uphos.append(pc.process_complete_pipeline(l, cutoff = 0, fasta_path=r'D:\\Projects\\Spectral libraries\\human.fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = PhosphoAnalysis()\n",
    "df_all_collapsed = builder.peptideCollapse(df_all, cutoff = 0, fasta_path = r'D:\\Projects\\Spectral libraries\\mouse.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = PhosphoAnalysis()\n",
    "df_stemcells_500cells_normalized_collapsed = builder.peptideCollapse(df_stemcells_500cells_normalized, cutoff = 0.75, fasta_path= r'D:\\Projects\\Spectral libraries\\mouse.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = get_cumulative_barplot(l_hela_sorted_collapsed, 3, point_size=9, point_color='black')\n",
    "#figure.write_image(r'D:\\figure.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Figure 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = get_cumulative_barplot(l_hela_sorted_collapsed_uphos, 3, point_size=9, point_color='black')\n",
    "\n",
    "#figure.write_image(r'D:\\Projects\\nanoPhos\\figures_upd\\figure3\\suppl_figure2a.pdf', height = 600, width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimated protein input is calculated based on the assumption that HeLa cells contain approximately 250 pg of total protein per cell.\n",
    "estimated_protein_input = [100*0.25, 300*0.25, 500*0.25, 1000*0.25, 2000*0.25, 3000*0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared_sorted_nanophos = calculate_interdilution_correlation(l_hela_sorted_collapsed, l_hela_sorted_collapsed[-1], estimated_protein_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=r_squared_sorted_nanophos,\n",
    "    nbinsx=len(r_squared_sorted_nanophos)//20,\n",
    "    name='PG Number',\n",
    "    marker=dict(\n",
    "        color='black',\n",
    "        line=dict(color='black', width=0.3)\n",
    "    ),\n",
    "    opacity=0.8\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    width=600, \n",
    "    height=600, \n",
    "    template='plotly_white',\n",
    "    xaxis_title='R squared',\n",
    "    yaxis_title='Count',\n",
    "    font=dict(size=12),\n",
    "    showlegend=False,\n",
    "    bargap=0.05\n",
    ")\n",
    "fig.add_vline(x = np.nanmedian(r_squared_sorted_nanophos), line = {'dash':'dash', 'width': 2, 'color':'black'})\n",
    "#fig.write_image(r'D:\\Projects\\nanoPhos\\figures_upd\\figure3\\figure3b.pdf', height = 600, width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Figure 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs_nanophos = []\n",
    "for df in l_hela_sorted_collapsed:\n",
    "    linear_values = np.power(2, df.iloc[:,:3])\n",
    "\n",
    "    n_valid = linear_values.notna().sum(axis=1)\n",
    "    \n",
    "    cv = linear_values.std(axis=1) / linear_values.mean(axis=1)\n",
    "    cv = cv.replace(0, np.nan)\n",
    "\n",
    "    cv[n_valid < 3] = np.nan\n",
    "    \n",
    "    cvs_nanophos.append(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs_nanophos_flattened = [item for sublist in cvs_nanophos for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2221857543191797"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmedian(cvs_nanophos_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_sequence_violet = ['#C79EEA', '#B178E2','#9B52DA','#7E2AC7','#6D25AD','#551D87']\n",
    "color_sequence_red = ['#FBA08D','#FA7A61','#F95534','#ED2E07','#CB2706','#9E1E05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i, el in enumerate(cvs_nanophos):\n",
    "    fig.add_trace(go.Box(y = el, marker_color = color_sequence_red[i]))\n",
    "    #fig.add_trace(go.Box(y = cvs_uphos[i], marker_color = color_sequence_violet[i]))\n",
    "fig.update_layout(width = 800, height = 600, template = 'none')\n",
    "fig.add_hline(y = np.nanmedian(cvs_nanophos_flattened), line = {'dash':'dash', 'width': 2, 'color':'black'})\n",
    "#fig.write_image(r'D:\\Projects\\nanoPhos\\figures_upd\\figure3\\supplfig2b.pdf', height = 600, width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Figure 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_100cells = pd.read_csv(r'Z:\\Denys_nanoPhos\\PRIDE\\analysis_data\\figure3\\selectivity_100cells.tsv', sep = '\\t')\n",
    "sel_300cells = pd.read_csv(r'Z:\\Denys_nanoPhos\\PRIDE\\analysis_data\\figure3\\selectivity_300cells.tsv', sep = '\\t')\n",
    "sel_500cells = pd.read_csv(r'Z:\\Denys_nanoPhos\\PRIDE\\analysis_data\\figure3\\selectivity_500cells.tsv', sep = '\\t')\n",
    "sel_1000cells = pd.read_csv(r'Z:\\Denys_nanoPhos\\PRIDE\\analysis_data\\figure3\\selectivity_1000cells.tsv', sep = '\\t')\n",
    "sel_2000cells = pd.read_csv(r'Z:\\Denys_nanoPhos\\PRIDE\\analysis_data\\figure3\\selectivity_2000cells.tsv', sep = '\\t')\n",
    "sel_3000cells = pd.read_csv(r'Z:\\Denys_nanoPhos\\PRIDE\\analysis_data\\figure3\\selectivity_3000cells.tsv', sep = '\\t')\n",
    "\n",
    "list_selectivity = [sel_100cells, sel_300cells, sel_500cells, sel_1000cells, sel_2000cells, sel_3000cells]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = []\n",
    "for df in list_selectivity:\n",
    "    sel.append(df.set_index('XLabel').apply(np.sum,axis = 1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_values = ['100', '300', '500', '1000', '2000', '3000']\n",
    "id_values1 = np.repeat(id_values, 3).tolist()\n",
    "flattened = [item for sublist in sel for item in sublist]\n",
    "df_selectivity = pd.DataFrame({'Selectivity':flattened, 'ID':id_values1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(df_selectivity, y = 'Selectivity', x = 'ID', orientation='h')\n",
    "fig.update_layout(width = 600, height = 600, template = 'plotly_white')\n",
    "fig.update_traces(marker = dict(size = 18, color = '#db4c2e', line = dict(width = 0.5, color = 'black')))\n",
    "fig.update_yaxes(\n",
    "    range=[0, 100],\n",
    "    showgrid=True,\n",
    "    gridwidth=0.1,           \n",
    "    gridcolor='#F3F2F2',  \n",
    "    griddash='solid'         \n",
    ")\n",
    "fig.write_image(r'D:\\Projects\\nanoPhos\\figures_upd\\figure3\\supplfig2c.pdf', height = 600, width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = []\n",
    "for i, el in enumerate(l_hela_sorted_collapsed):\n",
    "    tmp1 = []\n",
    "    tmp2 = []\n",
    "    for col in el.iloc[:,:3]:\n",
    "        tmp1.append(len(el[col].dropna()))\n",
    "    tmp1 = np.array(tmp1)\n",
    "    for col in l_hela_sorted_collapsed_uphos[i].iloc[:,:3]:\n",
    "        tmp2.append(len(l_hela_sorted_collapsed_uphos[i][col].dropna()))\n",
    "    tmp2 = np.array(tmp2)\n",
    "    ratio.append(np.round(tmp1/tmp2,2))\n",
    "id_values = ['100', '300', '500', '1000', '2000', '3000']\n",
    "id_values1 = np.repeat(id_values, 3).tolist()\n",
    "flattened = [item for sublist in ratio for item in sublist]\n",
    "df_ratio = pd.DataFrame({'Ratio':flattened, 'ID':id_values1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(df_ratio, y = 'Ratio', x = 'ID', orientation='h', log_y=True)\n",
    "fig.update_layout(width = 600, height = 600, template = 'plotly_white')\n",
    "fig.update_traces(marker = dict(size = 18, color = '#B3321E', line = dict(width = 0.5, color = 'black')))\n",
    "fig.update_yaxes(\n",
    "    range=[0, 1.1],\n",
    "    showgrid=True,\n",
    "    gridwidth=0.1,           \n",
    "    gridcolor='#F3F2F2',  \n",
    "    griddash='solid'         \n",
    ")\n",
    "#fig.write_image(r'D:\\figure.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_df = pd.DataFrame({'Sample': df_all['R.FileName'].unique().tolist(), 'Condition': ['2iL_100', '2iL_100', '2iL_100', 'SL_100','SL_100','SL_100', 'RA_100','RA_100','RA_100','RA24_100', 'RA24_100','RA24_100',\n",
    "                                                                                            '2iL_300', '2iL_300', '2iL_300', 'SL_300','SL_300','SL_300', 'RA_300','RA_300','RA_300','RA24_300', 'RA24_300','RA24_300',\n",
    "                                                                                            '2iL_500', '2iL_500', '2iL_500', 'SL_500','SL_500','SL_500', 'RA_500','RA_500','RA_500','RA24_500', 'RA24_500','RA24_500',\n",
    "                                                                                            '2iL_800', '2iL_800', '2iL_800', 'SL_800','SL_800','SL_800', 'RA_800','RA_800','RA_800','RA24_800', 'RA24_800','RA24_800',\n",
    "                                                                                            '2iL_1000', '2iL_1000', '2iL_1000', 'SL_1000','SL_1000','SL_1000', 'RA_1000','RA_1000','RA_1000','RA24_1000', 'RA24_1000','RA24_1000',\n",
    "                                                                                            '2iL_3000', '2iL_3000', '2iL_3000', 'SL_3000','SL_3000','SL_3000', 'RA_3000','RA_3000','RA_3000','RA24_3000', 'RA24_3000','RA24_3000']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_collapsed_cond = builder.assign_condition_setup(condition_df)\n",
    "df_all_collapsed_filt = df_all_collapsed_cond.loc[:, (1 - (df_all_collapsed_cond.isna().sum() / len(df_all_collapsed_cond))) >=0.64]\n",
    "df_all_collapsed_imp = ac.imputation_normal_distribution(df_all_collapsed_filt).reset_index()\n",
    "df_all_collapsed_imp = df_all_collapsed_imp[(df_all_collapsed_imp['group'].str.contains('RA24') == False)]\n",
    "pca = ac.run_pca(df_all_collapsed_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(pca[0][0], x = 'x', y = 'y', labels= \"sample\", color = 'group', color_discrete_sequence= ['#FBA08D', '#C79EEA', '#9ECCEA',\n",
    "                                                                                                           '#FA7A61', '#B178E2', '#78B8E2',\n",
    "                                                                                                           '#F95534', '#9B52DA', '#52A4DA',\n",
    "                                                                                                           '#ED2E07', '#7E2AC7', '#2A88C7' ,\n",
    "                                                                                                           '#CB2706', '#6D25AD', '#2576AD' ,\n",
    "                                                                                                           '#9E1E05', '#551D87', '#1D5C87' ])\n",
    "fig.update_layout(width = 600, height = 600, template = 'plotly_white')\n",
    "fig.update_traces(marker=dict(\n",
    "    size=21, \n",
    "    line=dict(width=1, color='black')\n",
    "))\n",
    "#fig.write_image(r'D:\\Projects\\nanoPhos\\figures_upd\\figure3\\fig3f.pdf', height = 600, width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = []\n",
    "for col in df_stemcells_500cells_normalized_collapsed.columns: \n",
    "    nums.append(len(df_stemcells_500cells_normalized_collapsed[col].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums1 = [nums[0:3], nums[4:7], nums[9:12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for el in nums1:\n",
    "    fig.add_trace(go.Box(y = el, boxpoints='all', pointpos=0))\n",
    "fig.update_layout(width = 500, height = 500, template = 'none')\n",
    "fig.update_yaxes(range = [0,25000])\n",
    "#fig.write_image(r'D:\\Projects\\nanoPhos\\figures_upd\\figure3\\fig3i.pdf', height = 500, width = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_stemcells_500cells_normalized_collapsed.iloc[:,[0,1,2,3,4,5,9,10,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "colspace = ['#e93526','#e93526','#e93526','#734b9e','#734b9e','#734b9e', '#2b89c7','#2b89c7','#2b89c7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i, col in enumerate(a.columns):\n",
    "    fig.add_trace(go.Violin(x = a[col].dropna().tolist(), marker_color = colspace[i]))\n",
    "median_val = a[col].dropna().median()\n",
    "fig.add_vline(x=median_val, line_dash=\"dash\", line_color='darkslategrey', line_width=2, opacity=0.7)\n",
    "fig.update_layout(width = 600, height = 500, template = 'none')\n",
    "fig.update_traces(orientation='h', side='positive', width=3, points=False, showlegend = False)\n",
    "#fig.write_image(r'D:\\Projects\\nanoPhos\\figures_upd\\figure3\\fig3j.pdf', height = 500, width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary figure 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_stemcells = [sorted_stemcells_100cells, sorted_stemcells_300cells, \n",
    "               sorted_stemcells_800cells, sorted_stemcells_1000cells, sorted_stemcells_3000cells]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_stemcells_collapsed = []\n",
    "pc = PeptideCollapse()\n",
    "for l in l_stemcells:\n",
    "    l_stemcells_collapsed.append(pc.process_complete_pipeline(l, cutoff = 0, add_kinase_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_grouped = []\n",
    "for df in l_stemcells_collapsed:\n",
    "    nums = []\n",
    "    for col in df.iloc[:,[0,1,2,3,4,5,9,10,11]].columns:\n",
    "        nums.append(len(df[col].dropna()))\n",
    "    nums_grouped.append([nums[:3], nums[3:6], nums[6:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [ '#ED2E07', '#7E2AC7', '#2A88C7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "fig = make_subplots(\n",
    "    rows=2, \n",
    "    cols=3,\n",
    "    subplot_titles=[f'Condition {i+1}' for i in range(5)],\n",
    "    specs=[[{}, {}, {}],\n",
    "           [{\"colspan\": 1}, {\"colspan\": 1}, None]],  \n",
    "    vertical_spacing=0.15,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "positions = [\n",
    "    (1, 1), (1, 2), (1, 3), \n",
    "    (2, 1), (2, 2)           \n",
    "]\n",
    "\n",
    "for idx, nums in enumerate(nums_grouped):\n",
    "    row, col = positions[idx]\n",
    "    \n",
    "    all_data = [val for replicate in nums for val in replicate]\n",
    "    median_val = np.median(all_data)\n",
    "    y_max = median_val * 2  \n",
    "\n",
    "    for i, el in enumerate(nums):\n",
    "        fig.add_trace(\n",
    "            go.Box(y=el, boxpoints='all', pointpos=0, showlegend=False,\n",
    "                   marker_color=colors[i], line_color=colors[i]),\n",
    "            row=row, \n",
    "            col=col\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(range=[0, y_max], row=row, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    template='none',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "#fig.write_image(r'D:\\Projects\\nanoPhos\\figures_upd\\figure3\\supplfig2e.pdf', height = 800, width = 1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemcells_500cells_normalized_collapsed_filt = df_stemcells_500cells_normalized_collapsed.loc[:, (1 - (df_stemcells_500cells_normalized_collapsed.isna().sum() / len(df_stemcells_500cells_normalized_collapsed))) >=0.7] # filtering 70% missing\n",
    "df_stemcells_500cells_normalized_collapsed_imp = ac.imputation_normal_distribution(df_stemcells_500cells_normalized_collapsed_filt).reset_index() #imputation\n",
    "df_stemcells_500cells_normalized_collapsed_imp = df_stemcells_500cells_normalized_collapsed_imp[df_stemcells_500cells_normalized_collapsed_imp['group'] != 'RA24'] # removing RA24 samples for this analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anova = ac.run_anova(df_stemcells_500cells_normalized_collapsed_imp) # ANOVA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemcells_500cells_normalized_collapsed_imp_tmp = df_stemcells_500cells_normalized_collapsed_imp.drop(['sample', 'subject'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_genes = pd.read_excel(r'D:\\Marker genes.xlsx') #list of marker transcription genes for stem cell differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_include = ['Pou3f1', 'Dnmt3a', 'Dbn1', 'Krt18', 'Sox3', 'Esrrb', 'Sox2', 'Otx2', 'Dnmt3b', 'Pou5f1', 'Dnmt3l', 'Tet2', 'Nanog', 'Nes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_df_sig = df_anova[df_anova['rejected'] == True]\n",
    "meta = df_stemcells_500cells_normalized_collapsed_imp[['group', 'sample', 'subject']]\n",
    "df_stemcells_500cells_normalized_collapsed_imp = df_stemcells_500cells_normalized_collapsed_imp.set_index('sample').drop(['group', 'subject'], axis = 1).T.reset_index()\n",
    "df_stemcells_500cells_normalized_collapsed_imp = df_stemcells_500cells_normalized_collapsed_imp[df_stemcells_500cells_normalized_collapsed_imp ['PTM_Collapse_key'].isin(anova_df_sig['identifier'])]\n",
    "df_stemcells_500cells_normalized_collapsed_imp['Gene'] = df_stemcells_500cells_normalized_collapsed_imp['PTM_Collapse_key'].apply(lambda x: x.split('~')[1]).apply(lambda x: x.split('_')[0])\n",
    "df_stemcells_500cells_normalized_collapsed_imp = df_stemcells_500cells_normalized_collapsed_imp[df_stemcells_500cells_normalized_collapsed_imp['Gene'].isin(to_include)].drop('Gene',axis = 1)\n",
    "df_stemcells_500cells_normalized_collapsed_imp = df_stemcells_500cells_normalized_collapsed_imp.set_index('PTM_Collapse_key')\n",
    "df_norm = z_normalize_data(df_stemcells_500cells_normalized_collapsed_imp)\n",
    "linkage_samples, linkage_features = perform_hierarchical_clustering(\n",
    "        df_norm, method='ward', metric='euclidean'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustermap = plot_clustermap(df_norm, figsize=(8, 7), method='ward', \n",
    "                                metric='euclidean', n_clusters=1, save_path=r'D:\\Projects\\nanoPhos\\images_export\\img10.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoPhos_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
